# Kubernetes

# O quÃª preciso saber antes de comeÃ§ar?

## Qual distro GNU/Linux devo usar?

Devido ao fato de algumas ferramentas importantes, como o ``systemd`` e ``journald``, terem se tornado padrÃ£o na maioria das principais distribuiÃ§Ãµes disponÃ­veis hoje, vocÃª nÃ£o deve encontrar problemas para seguir o treinamento, caso vocÃª opte por uma delas, como Ubuntu, Debian, CentOS e afins.

## Alguns sites que devemos visitar

- [https://kubernetes.io](https://kubernetes.io)

- [https://github.com/kubernetes/kubernetes/](https://github.com/kubernetes/kubernetes/)

- [https://12factor.net/pt_br/](https://12factor.net/pt_br/)

## E o k8s?

**VersÃ£o resumida:**

O projeto Kubernetes foi desenvolvido pela Google, em meados de 2014, para atuar como um orquestrador de contÃªineres para a empresa. O Kubernetes (k8s), cujo termo em Grego significa "timoneiro", Ã© um projeto *opensource* que conta com *design* e desenvolvimento baseados no projeto Borg, que tambÃ©m Ã© da Google [1](https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/). Alguns outros produtos disponÃ­veis no mercado, tais como o Apache Mesos e o Cloud Foundry, tambÃ©m surgiram a partir do projeto Borg.

Como Kubernetes Ã© uma palavra difÃ­cil de se pronunciar - e de se escrever - a comunidade simplesmente o apelidou de **k8s**, seguindo o padrÃ£o [i18n](http://www.i18nguy.com/origini18n.html) (a letra "k" seguida por oito letras e o "s" no final), pronunciando-se simplesmente "kates".

## Arquitetura do k8s

Assim como os demais orquestradores disponÃ­veis, o k8s tambÃ©m segue um modelo *master/worker*, constituindo assim um *cluster*, onde para seu funcionamento devem existir no mÃ­nimo trÃªs nÃ³s: o nÃ³ *master*, responsÃ¡vel (por padrÃ£o) pelo gerenciamento do *cluster*, e os demais como *workers*, executores das aplicaÃ§Ãµes que queremos executar sobre esse *cluster*.

Embora exista a exigÃªncia de no mÃ­nimo trÃªs nÃ³s para a execuÃ§Ã£o do k8s em um ambiente padrÃ£o, existem soluÃ§Ãµes para se executar o k8s em um Ãºnico nÃ³. Alguns exemplos sÃ£o:

* [Kind](https://kind.sigs.k8s.io/docs/user/quick-start): Uma ferramenta para execuÃ§Ã£o de contÃªineres Docker que simulam o funcionamento de um cluster Kubernetes. Ã‰ utilizado para fins didÃ¡ticos, de desenvolvimento e testes. O **Kind nÃ£o deve ser utilizado para produÃ§Ã£o**;

* [Minikube](https://github.com/kubernetes/minikube): ferramenta para implementar um *cluster* Kubernetes localmente com apenas um nÃ³. Muito utilizado para fins didÃ¡ticos, de desenvolvimento e testes. O **Minikube nÃ£o deve ser utilizado para produÃ§Ã£o**;

* [MicroK8S](https://microk8s.io): Desenvolvido pela [Canonical](https://canonical.com), mesma empresa que desenvolve o [Ubuntu](https://ubuntu.com). Pode ser utilizado em diversas distribuiÃ§Ãµes e **pode ser utilizada para ambientes de produÃ§Ã£o**, em especial para *Edge Computing* e IoT (*Internet of things*);

* [k3s](https://k3s.io): Desenvolvido pela [Rancher Labs](https://rancher.com), Ã© um concorrente direto do MicroK8s, podendo ser executado inclusive em Raspberry Pi.

A figura a seguir mostra a arquitetura interna de componentes do k8s.

| ![Arquitetura Kubernetes](../images/kubernetes_architecture.png) |
|:---------------------------------------------------------------------------------------------:|
| *Arquitetura Kubernetes [Ref: phoenixnap.com KB article](https://phoenixnap.com/kb/understanding-kubernetes-architecture-diagrams)*                                                                      |

* **API Server**: Ã‰ um dos principais componentes do k8s. Este componente fornece uma API que utiliza JSON sobre HTTP para comunicaÃ§Ã£o, onde para isto Ã© utilizado principalmente o utilitÃ¡rio ``kubectl``, por parte dos administradores, para a comunicaÃ§Ã£o com os demais nÃ³s, como mostrado no grÃ¡fico. Estas comunicaÃ§Ãµes entre componentes sÃ£o estabelecidas atravÃ©s de requisiÃ§Ãµes [REST](https://restfulapi.net);

* **etcd**: O etcd Ã© um *datastore* chave-valor distribuÃ­do que o k8s utiliza para armazenar as especificaÃ§Ãµes, status e configuraÃ§Ãµes do *cluster*. Todos os dados armazenados dentro do etcd sÃ£o manipulados apenas atravÃ©s da API. Por questÃµes de seguranÃ§a, o etcd Ã© por padrÃ£o executado apenas em nÃ³s classificados como *master* no *cluster* k8s, mas tambÃ©m podem ser executados em *clusters* externos, especÃ­ficos para o etcd, por exemplo;

* **Scheduler**: O *scheduler* Ã© responsÃ¡vel por selecionar o nÃ³ que irÃ¡ hospedar um determinado *pod* (a menor unidade de um *cluster* k8s - nÃ£o se preocupe sobre isso por enquanto, nÃ³s falaremos mais sobre isso mais tarde) para ser executado. Esta seleÃ§Ã£o Ã© feita baseando-se na quantidade de recursos disponÃ­veis em cada nÃ³, como tambÃ©m no estado de cada um dos nÃ³s do *cluster*, garantindo assim que os recursos sejam bem distribuÃ­dos. AlÃ©m disso, a seleÃ§Ã£o dos nÃ³s, na qual um ou mais pods serÃ£o executados, tambÃ©m pode levar em consideraÃ§Ã£o polÃ­ticas definidas pelo usuÃ¡rio, tais como afinidade, localizaÃ§Ã£o dos dados a serem lidos pelas aplicaÃ§Ãµes, etc;

* **Controller Manager**: Ã‰ o *controller manager* quem garante que o *cluster* esteja no Ãºltimo estado definido no etcd. Por exemplo: se no etcd um *deploy* estÃ¡ configurado para possuir dez rÃ©plicas de um *pod*, Ã© o *controller manager* quem irÃ¡ verificar se o estado atual do *cluster* corresponde a este estado e, em caso negativo, procurarÃ¡ conciliar ambos;

* **Kubelet**: O *kubelet* pode ser visto como o agente do k8s que Ã© executado nos nÃ³s workers. Em cada nÃ³ worker deverÃ¡ existir um agente Kubelet em execuÃ§Ã£o. O Kubelet Ã© responsÃ¡vel por de fato gerenciar os *pods*, que foram direcionados pelo *controller* do *cluster*, dentro dos nÃ³s, de forma que para isto o Kubelet pode iniciar, parar e manter os contÃªineres e os pods em funcionamento de acordo com o instruÃ­do pelo controlador do cluster;

* **Kube-proxy**: Age como um *proxy* e um *load balancer*. Este componente Ã© responsÃ¡vel por efetuar roteamento de requisiÃ§Ãµes para os *pods* corretos, como tambÃ©m por cuidar da parte de rede do nÃ³;

* **Container Runtime**: O *container runtime* Ã© o ambiente de execuÃ§Ã£o de contÃªineres necessÃ¡rio para o funcionamento do k8s. Em 2016 suporte ao [rkt](https://coreos.com/rkt/) foi adicionado, porÃ©m desde o inÃ­cio o Docker jÃ¡ Ã© funcional e utilizado por padrÃ£o.

## Portas que devemos nos preocupar

**MASTER**

Protocol|Direction|Port Range|Purpose|Used By
--------|---------|----------|-------|-------
TCP|Inbound|6443*|Kubernetes API server|All
TCP|Inbound|2379-2380|etcd server client API|kube-apiserver, etcd
TCP|Inbound|10250|Kubelet API|Self, Control plane
TCP|Inbound|10251|kube-scheduler|Self
TCP|Inbound|10252|kube-controller-manager|Self

* Toda porta marcada por * Ã© customizÃ¡vel, vocÃª precisa se certificar que a porta alterada tambÃ©m esteja aberta.

**WORKERS**

Protocol|Direction|Port Range|Purpose|Used By
--------|---------|----------|-------|-------
TCP|Inbound|10250|Kubelet API|Self, Control plane
TCP|Inbound|30000-32767|NodePort|Services All

Caso vocÃª opte pelo [Weave](https://weave.works) como *pod network*, devem ser liberadas tambÃ©m as portas 6783 e 6784 TCP.

## TÃ¡, mas qual tipo de aplicaÃ§Ã£o eu devo rodar sobre o k8s?

O melhor *app* para executar em contÃªiner, principalmente no k8s, sÃ£o aplicaÃ§Ãµes que seguem o [The Twelve-Factor App](https://12factor.net/pt_br/).

## Conceitos-chave do k8s

Ã‰ importante saber que a forma como o k8s gerencia os contÃªineres Ã© ligeiramente diferente de outros orquestradores, como o Docker Swarm, sobretudo devido ao fato de que ele nÃ£o trata os contÃªineres diretamente, mas sim atravÃ©s de *pods*. Vamos conhecer alguns dos principais conceitos que envolvem o k8s a seguir:

- **Pod**: Ã© o menor objeto do k8s. Como dito anteriormente, o k8s nÃ£o trabalha com os contÃªineres diretamente, mas organiza-os dentro de *pods*, que sÃ£o abstraÃ§Ãµes que dividem os mesmos recursos, como endereÃ§os, volumes, ciclos de CPU e memÃ³ria. Um pod, embora nÃ£o seja comum, pode possuir vÃ¡rios contÃªineres;

- **Controller**: Ã© o objeto responsÃ¡vel por interagir com o *API Server* e orquestrar algum outro objeto. Exemplos de objetos desta classe sÃ£o os *Deployments* e *Replication Controllers*;

- **ReplicaSets**: Ã© um objeto responsÃ¡vel por garantir a quantidade de pods em execuÃ§Ã£o no nÃ³;

- **Deployment**: Ã‰ um dos principais *controllers* utilizados. O *Deployment*, em conjunto com o *ReplicaSet*, garante que determinado nÃºmero de rÃ©plicas de um pod esteja em execuÃ§Ã£o nos nÃ³s workers do cluster. AlÃ©m disso, o Deployment tambÃ©m Ã© responsÃ¡vel por gerenciar o ciclo de vida das aplicaÃ§Ãµes, onde caracterÃ­sticas associadas a aplicaÃ§Ã£o, tais como imagem, porta, volumes e variÃ¡veis de ambiente, podem ser especificados em arquivos do tipo *yaml* ou *json* para posteriormente serem passados como parÃ¢metro para o ``kubectl`` executar o deployment. Esta aÃ§Ã£o pode ser executada tanto para criaÃ§Ã£o quanto para atualizaÃ§Ã£o e remoÃ§Ã£o do deployment;

- **Jobs e CronJobs**: sÃ£o objetos responsÃ¡veis pelo gerenciamento de jobs isolados ou recorrentes.

# Aviso sobre os comandos

> AtenÃ§Ã£o!!! Antes de cada comando Ã© apresentado o tipo prompt. Exemplos:

```
$ comando1
```

```
# comando2
```

> O prompt que inicia com o caractere "$", indica que o comando deve ser executado com um usuÃ¡rio comum do sistema operacional.
>
> O prompt que inicia com o caractere "#", indica que o comando deve ser executado com o usuÃ¡rio **root**.
>
> VocÃª nÃ£o deve copiar/colar o prompt, apenas o comando. :-)

# Minikube

## Requisitos bÃ¡sicos

Ã‰ importante frisar que o Minikube deve ser instalado localmente, e nÃ£o em um *cloud provider*. Por isso, as especificaÃ§Ãµes de *hardware* a seguir sÃ£o referentes Ã  mÃ¡quina local.

* Processamento: 1 core;
* MemÃ³ria: 2 GB;
* HD: 20 GB.

## InstalaÃ§Ã£o do Minikube no GNU/Linux

Antes de mais nada, verifique se a sua mÃ¡quina suporta virtualizaÃ§Ã£o. No GNU/Linux, isto pode ser realizado com:

```
grep -E --color 'vmx|svm' /proc/cpuinfo
```

Caso a saÃ­da do comando nÃ£o seja vazia, o resultado Ã© positivo.

ApÃ³s isso, vamos instalar o ``kubectl`` com os seguintes comandos.

```
curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl

chmod +x ./kubectl

sudo mv ./kubectl /usr/local/bin/kubectl

kubectl version --client
```

HÃ¡ a possibilidade de nÃ£o utilizar um *hypervisor* para a instalaÃ§Ã£o do Minikube, executando-o ao invÃ©s disso sobre o prÃ³prio host. Iremos utilizar o Oracle VirtualBox como *hypervisor*, que pode ser encontrado [aqui](https://www.virtualbox.org).

Efetue o download e a instalaÃ§Ã£o do ``Minikube`` utilizando os seguintes comandos.

```
curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64

chmod +x ./minikube

sudo mv ./minikube /usr/local/bin/minikube

minikube version
```

## kubectl: alias e autocomplete

Execute o seguinte comando para configurar o alias e autocomplete para o ``kubectl``.

No Bash:

```bash
source <(kubectl completion bash) # configura o autocomplete na sua sessÃ£o atual (antes, certifique-se de ter instalado o pacote bash-completion).

echo "source <(kubectl completion bash)" >> ~/.bashrc # add autocomplete permanentemente ao seu shell.
```

## Iniciando, parando e excluindo o Minikube

Quando operando em conjunto com um *hypervisor*, o Minikube cria uma mÃ¡quina virtual, onde dentro dela estarÃ£o todos os componentes do k8s para execuÃ§Ã£o. Para realizar a inicializaÃ§Ã£o desse ambiente, antes de executar o minikube, precisamos setar o VirtualBox como padrÃ£o para subir este ambiente, para que isso aconteÃ§a execute o comando:

```
minikube config set driver virtualbox
```

Caso nÃ£o queria deixar o VirtualBox como padrÃ£o sempre que subir o ambiente novo, vocÃª deve digitar o comando ``minikube start --driver=virtualbox``. Mas como jÃ¡ setamos o VirtualBox como padrÃ£o para subir o ambiente do minikube, basta executar:

```
minikube start
```

Para criar um cluster com multi-node basta executar:

``` 
minikube start --nodes 2 -p multinode-demo
```

Caso deseje parar o ambiente:

```
minikube stop
```

Para excluir o ambiente:

```
minikube delete
```

## Certo, e como eu sei que estÃ¡ tudo funcionando como deveria?

Uma vez iniciado, vocÃª deve ter uma saÃ­da na tela similar Ã  seguinte:

```
minikube start


ğŸ‰  minikube 1.10.0 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.10.0
ğŸ’¡  To disable this notice, run: 'minikube config set WantUpdateNotification false'

ğŸ™„  minikube v1.9.2 on Darwin 10.11
âœ¨  Using the virtualbox driver based on existing profile
ğŸ‘  Starting control plane node m01 in cluster minikube
ğŸ”„  Restarting existing virtualbox VM for "minikube" ...
ğŸ³  Preparing Kubernetes v1.19.1 on Docker 19.03.8 ...
ğŸŒŸ  Enabling addons: default-storageclass, storage-provisioner
ğŸ„  Done! kubectl is now configured to use "minikube"
```

VocÃª pode entÃ£o listar os nÃ³s que fazem parte do seu *cluster* k8s com o seguinte comando:

```
kubectl get nodes
```

A saÃ­da serÃ¡ similar ao conteÃºdo a seguir:

Para um node:

```
kubectl get nodes

NAME       STATUS   ROLES    AGE   VERSION
minikube   Ready    master   8d    v1.19.1
```

Para multi-nodes:

```
NAME                 STATUS    ROLES     AGE       VERSION
multinode-demo       Ready     master    5m        v1.19.1
multinode-demo-m02   Ready     <none>    4m        v1.19.1
```

Inicialmente, a intenÃ§Ã£o do Minikube Ã© executar o k8s em apenas um nÃ³, porÃ©m a partir da versÃ£o 1.10.1 e possÃ­vel usar a funÃ§Ã£o de multi-node (Experimental).

Caso os comandos anteriores tenham sido executados sem erro, a instalaÃ§Ã£o do Minikube terÃ¡ sido realizada com sucesso.

## Descobrindo o endereÃ§o do Minikube

Como dito anteriormente, o Minikube irÃ¡ criar uma mÃ¡quina virtual, assim como o ambiente para a execuÃ§Ã£o do k8s localmente. Ele tambÃ©m irÃ¡ configurar o ``kubectl`` para comunicar-se com o Minikube. Para saber qual Ã© o endereÃ§o IP dessa mÃ¡quina virtual, pode-se executar:

```
minikube ip
```

O endereÃ§o apresentado Ã© que deve ser utilizado para comunicaÃ§Ã£o com o k8s.

## Acessando a mÃ¡quina do Minikube via SSH

Para acessar a mÃ¡quina virtual criada pelo Minikube, pode-se executar:

```
minikube ssh
```

## Dashboard

O Minikube vem com um *dashboard* *web* interessante para que o usuÃ¡rio iniciante observe como funcionam os *workloads* sobre o k8s. Para habilitÃ¡-lo, o usuÃ¡rio pode digitar:

```
minikube dashboard
```

## Logs

Os *logs* do Minikube podem ser acessados atravÃ©s do seguinte comando.

```
minikube logs
```
